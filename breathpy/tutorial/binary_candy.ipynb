{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial demonstrates the binary analysis workflow of `BreathPy`.\n",
    "Initially sample data (MCC-IMS measurements of breath after consuming either `menthol` or `citrus` candy, see material section of ([Weber et al. 2020][Weber2020]) for more information) is downloaded and split into a training and testing fraction - the test samples will later serve to test the created random forest classifier. The samples are normalized and denoised. Afterwards, several peak-detection methods are applied including the manuallly created `VisualnowLayer` contained in the sample data. Subsequently, peaks are aligned using the `ProbeClustering` approach and the features are reduced using `RemovePercentageFeatures` - which limits the reported features to the ones present in at least `percentage_threshold` of the minority class - in this case `citrus`. \n",
    "Features are then weighted using the `PerformanceMeasure`s `RANDOM_FOREST_CLASSIFICATION` and `FDR_CORRECTED_P_VALUE` - leaving 10 features each. \n",
    "Two decision trees and a `RandomForestClassifier` are trained. They serve as visual interpretation of the classifcation strategy based on each `PerformanceMeasure`.\n",
    "After training, the `RandomForestClassifier` is used to predict the class labels of the test samples.\n",
    "Finally, plots are created and saved in the `results/plots/` directory.\n",
    "\n",
    "[Weber2020]: https://doi.org/10.3390/metabo10100393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings for rendering as pdf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# handle imports\n",
    "from urllib.request import urlretrieve\n",
    "from shutil import move as file_move\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from breathpy.generate_sample_data import generate_train_test_sets, generate_train_test_set_helper\n",
    "from breathpy.model.BreathCore import (MccImsAnalysis, MccImsMeasurement, PredictionModel,\n",
    "                              construct_default_parameters,\n",
    "                              construct_default_processing_evaluation_steps,\n",
    "                              construct_custom_processing_evaluation_dict)\n",
    "from breathpy.model.ProcessingMethods import FeatureReductionMethod, PerformanceMeasure, GCMSPeakDetectionMethod, GCMSAlignmentMethod\n",
    "from breathpy.tools.tools import get_peax_binary_path\n",
    "\n",
    "from breathpy.view.BreathVisualizations import ClusterPlot, HeatmapPlot, RocCurvePlot, BoxPlot, TreePlot, TimeSeriesPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample data and split into train and test fraction\n",
    "url = 'https://github.com/philmaweb/BreathAnalysis.github.io/raw/master/data/full_candy.zip'\n",
    "zip_dst = Path(\"data/full_candy.zip\")\n",
    "dst_dir = Path(\"data/full_candy/\")\n",
    "dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "urlretrieve(url, zip_dst)\n",
    "\n",
    "# unzip archive into data subdirectory\n",
    "with ZipFile(zip_dst, \"r\") as archive_handle:\n",
    "    archive_handle.extractall(Path(dst_dir))\n",
    "\n",
    "raw_dir = dst_dir\n",
    "target_dir = Path(\"data/\")\n",
    "\n",
    "# split into train and test fraction - use 1/3 of samples for validation\n",
    "_ = generate_train_test_sets(dir_full_set=raw_dir, root_target_dir=target_dir, cross_val_num=3, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we simplify and go step-by-step through the methods called by `breathpy.model.CoreTest.run_start_to_end_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# capture print statements of comandline output \n",
    "\n",
    "# define default parameters and train / test directory\n",
    "folder_name = file_prefix = 'train_full_candy'\n",
    "plot_parameters, file_parameters = construct_default_parameters(file_prefix, folder_name, make_plots=True)\n",
    "\n",
    "# create default parameters for preprocessing and evaluation\n",
    "preprocessing_steps, evaluation_params_dict = construct_default_processing_evaluation_steps()\n",
    "\n",
    "# define directory for training and test set\n",
    "train_dir = Path(\"data/train_full_candy/\")\n",
    "test_dir = Path(\"data/test_full_candy/\")\n",
    "\n",
    "# get class label dict file from training set\n",
    "train_class_label_dict_fn = MccImsAnalysis.guess_class_label_extension(train_dir)\n",
    "\n",
    "# read in raw mcc-ims measurements of training set - based on class_label_dict\n",
    "train_measurements = [MccImsMeasurement(fn) for fn in train_dir.glob(\"*ims.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# capture print statements of comandline output \n",
    "\n",
    "# setup analysis - need to get path of peax binary and get the visualnowlayer filename\n",
    "visualnow_layer_path = Path(\"data/train_full_candy/candy_layer.xls\")\n",
    "\n",
    "# create output directory\n",
    "if not Path(file_parameters['out_dir']).exists():\n",
    "    Path(file_parameters['out_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create analysis\n",
    "ims_analysis = MccImsAnalysis(\n",
    "    train_measurements, preprocessing_steps, performance_measure_parameters=evaluation_params_dict,\n",
    "    class_label_file=train_class_label_dict_fn, dataset_name=folder_name, visualnow_layer_file=visualnow_layer_path,\n",
    "    peax_binary_path=get_peax_binary_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Upon initialization with the raw training samples and associated class labels, the raw measurements are denoised and normalized using the default parameters.\n",
    "\n",
    "Normalization and denoising methods are applied to reduce technical artifacts and instrumental noise and standardize the intensity range for measurements ([Engel et al. 2013][Engel2013]). Previous comprehensive studies of preprocessing methods for MCC-IMS data have highlighted, compared and discussed the best performing methods in detail in ([Smolinska et al. 2014][Smolinska2014], [Szymanska et al. 2016][Szymanska2016], [Horsch et al. 2017][Horsch2017]).\n",
    "\n",
    "The default normalization methods are `INTENSITY_NORMALIZATION` and `BASELINE_CORRECTION`. They are both designed to reduce the influence of the reactant ion peak (RIP), which is a disturbance that appears in every MCC-IMS measurement at a characteristic inverse reduced mobility (IRM) position ([Smolinska et al. 2014][Smolinska2014]). As it influences the spectrum's intensity at the same retention time (RT), we try to remove its influence and make the intensity values more comparable. \n",
    "The `BASELINE_CORRECTION` method reduces the baseline of the spectra affected by the RIP by subtracting the 25%-quantile of the intensity (see ([Szymanska et al. 2016][Szymanska2016]) for more details), while the `INTENSITY_NORMALIZATION` method normalizes the intensity values based on the maximum intensity of the RIP.\n",
    "\n",
    "Default denoising methods are `CROP_INVERSE_REDUCED_MOBILITY`, `DISCRETE_WAVELET_TRANSFORMATION`, `SAVITZKY_GOLAY_FILTER`, `MEDIAN_FILTER`, and `GAUSSIAN_FILTER`. `CROP_INVERSE_REDUCED_MOBILITY` removes the parts of all spectra that have IRM values below 0.4. \n",
    "`DISCRETE_WAVELET_TRANSFORMATION` compresses and reconstructs the spectra, for which we use the PyWavelets ([Lee et al. 2019][Lee2019]) implementation. The intuition behind this is that the compression will remove noise from the spectrum and only reconstruct the relevant signal.\n",
    "\n",
    "`SAVITZKY_GOLAY_FILTER`, `MEDIAN_FILTER`, and `GAUSSIAN_FILTER` smooth the spectra using the neighboring intensity values, the weighted average intensity, or using a kernel method, which should remove noise, but will likely dampen the signal. They were implemented using SciPy ([Virtanen et al. 2020][Virtanen2020]).\n",
    "\n",
    "Subsequently, the default peak-detection methods `PEAX` ([D'Addario et al. 2014][DAddario2014]\n",
    "), `WATERSHED` ([Bunkowski et al. 2019][Bunkowski2019]), `VISUALNOWLAYER` ([Bödeker et al. 2008][Bödeker2008]), and `TOPHAT` ([Sternberg et al. 1986][Sternberg1986], [Virtanen et al. 2020][Virtanen2020]) are applied, resulting in the creation of a `PeakDetectionResult` for every sample and used peak-detection method. \n",
    "\n",
    "The elimination of manual peak-detection for MCC-IMS data and the development of automated methods have been extensively studied over the past decade ([Horsch et al. 2017][Horsch2017]), which allowed us to integrate successful tools and comparable approaches. The `VISUALNOW_LAYER` method extracts peaks based on provided coordinates, which are imported from an annotation file of the commercial Visual Now software (B&S Analytik, Dortmund, Germany) ([Bödeker et al. 2008][Bödeker2008]).\n",
    "\n",
    "\n",
    "[Engel2013]: https://doi.org/10.1016/j.trac.2013.04.015\n",
    "[Smolinska2014]: https://doi.org/10.1088/1752-7155/8/2/027105\n",
    "[Szymanska2016]: https://doi.org/10.1039/C6AN01008C\n",
    "[Horsch2017]: https://doi.org/10.1371/journal.pone.0184321\n",
    "[Lee2019]: https://doi.org/10.21105/joss.01237\n",
    "[Virtanen2020]: https://doi.org/10.1038/s41592-019-0686-2\n",
    "[DAddario2014]: https://doi.org/10.1186/1471-2105-15-25\n",
    "[Bunkowski2019]:https://pub.uni-bielefeld.de/publication/2517237\n",
    "[Bödeker2008]: https://doi.org/10.1007/s12127-008-0012-7\n",
    "[Sternberg1986]: http://www.sciencedirect.com/science/article/pii/0734189X86900046\n",
    "[Weber2020]: https://doi.org/10.3390/metabo10100393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# run normalization, denoising and peak_detection for measurements using 6 cores\n",
    "# for peak_detection we run [PEAX, WATERSHED, VISUALNOWLAYER, TOPHAT] methods defined in preprocessing_steps\n",
    "# if one want to change default parameters, pass updated parameters for preprocessing_parameters\n",
    "ims_analysis.preprocess_multicore(num_cores=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**  \n",
    "\n",
    "* Plot and show a chromatogram after normalization and denoising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and show a preprocessed measurement\n",
    "test_measurement = ims_analysis.measurements[0]\n",
    "HeatmapPlot.FastIntensityMatrix(test_measurement, plot_parameters=plot_parameters, title=str(test_measurement))\n",
    "\n",
    "Image(Path(\"results/plots/heatmaps/fast_train_full_candy_intentsity_plot_BD18_1408280834_ims.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak-Alignment\n",
    "The implemented peak-alignment methods are `PROBE_CLUSTERING` and `DBSCAN` ([Ester et al. 1996][Ester1996]), which covered in detail in ([Weber et al. 2020][Weber2020]). The default method for peak-alignment is `PROBE_CLUSTERING`.\n",
    "\n",
    "[Ester1996]: http://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf\n",
    "[Weber2020]: https://doi.org/10.3390/metabo10100393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align peak detection results\n",
    "ims_analysis.align_peaks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "* Plot and show clusters for each peak detection method\n",
    "* Plot and show average chromatogram for each candy type - after normalization and denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = ClusterPlot.ClusterBasic(ims_analysis, plot_parameters=plot_parameters)\n",
    "overlays = ClusterPlot.OverlayClasswiseAlignment(ims_analysis, plot_parameters=plot_parameters)\n",
    "\n",
    "# get paths of the images\n",
    "cluster_fn = Path(clusters[2][-1])\n",
    "overlay_fn_citrus =Path(\"results/plots/overlay/\")/overlays[2][-1]\n",
    "overlay_fn_menthol =Path(\"results/plots/overlay/\")/overlays[-2][-1]\n",
    "\n",
    "# display images for the TOPHAT method\n",
    "images = [cluster_fn, overlay_fn_citrus, overlay_fn_menthol]\n",
    "\n",
    "for fn in images:\n",
    "    display(Image(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-Reduction\n",
    "After peak-alignment, feature-reduction can be applied. In this process, the features are filtered using the `REMOVE_PERCENTAGE_FEATURES` method, which removes features that are lower than a `noise_threshold` or that are not represented in a certain percentage of samples.\n",
    "The default `noise_threshold` is a permissive 0.0001, while the `percentage_threshold` requires that at least 50% of samples in any sample show the feature.\n",
    "\n",
    "### Performance Evaluation\n",
    "The result of feature-reduction are reduced sample-by-feature matrices.\n",
    "Based on these matrices, a random forest classifier is trained and evaluated using three-fold cross-validation. During this process, features are ranked and selected for both performance measures `FDR_CORRECTED_P_VALUE` and `RANDOM_FOREST_CLASSIFICATION`.\n",
    "False discovery rate corrected p-values are computed using the `FDR_CORRECTED_P_VALUE` method, which calculates p-values based on the Mann-Whitney-U test ([Mann and Whitney 1947][Mann1947]) and adjusted for multiple testing using the Benjamini-Hochberg procedure ([Benjamini and Hochberg 1995][Benjamini1995]). For our calculations, we use the implementations of statsmodels ([Seabold and Perktold 2010][Seabold2010]). \n",
    "In `RANDOM_FOREST_CLASSIFICATION`, the features are weighted based on the reported feature importance over the cross-validation loops. \n",
    "\n",
    "[Mann1947]: https://doi.org/10.1214/aoms/1177730491\n",
    "[Benjamini1995]: https://doi.org/10.1111/j.2517-6161.1995.tb02031.x\n",
    "[Seabold2010]: http://statsmodels.sourceforge.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply feature reduction\n",
    "ims_analysis.reduce_features(ims_analysis.AVAILABLE_FEATURE_REDUCTION_METHODS)\n",
    "# evaluate model performance using 3-fold cross-validation\n",
    "ims_analysis.evaluate_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "In order to speed up future analysis or enable the analysis of intermediary results, we implemented an export and import functionality.\n",
    "In this case, preprocessed measurements, peak detection results and feature_matrices are exported in the `.csv` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# export preprocessed files, peak detection results and feature_matrixes to csv into results directory\n",
    "print(file_parameters['out_dir'])\n",
    "\n",
    "# export preprocessed files, peak detection results and feature_matrixes to csv into results directory\n",
    "ims_analysis.export_results_to_csv(file_parameters['out_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the analysis:\n",
    "* Plot and show estimated model performance - ROC curve\n",
    "* Plot and show best features superimposed for each candy type\n",
    "* Plot and and show boxplot and time-series for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_plots = RocCurvePlot.ROCCurve(ims_analysis.analysis_result, plot_parameters=plot_parameters)\n",
    "box_plots = BoxPlot.BoxPlotBestFeature(ims_analysis.analysis_result, plot_parameters=plot_parameters)\n",
    "\n",
    "try:\n",
    "    dt_plots = TreePlot.DecisionTrees(ims_analysis.analysis_result, plot_parameters=plot_parameters, limit_to_peak_detection_method_name=\"TOPHAT\")\n",
    "except FileNotFoundError as e:\n",
    "    # might not both be installe - need system executable and python library\n",
    "    print(\"Probably graphviz is not installed - install via `conda install graphviz python-graphviz`\")\n",
    "    raise(e)\n",
    "\n",
    "ts_plots = TimeSeriesPlot.TimeSeriesFromAnalysis(ims_analysis, plot_parameters=plot_parameters, limit_to_pdmn=['TOPHAT'], limit_to_features=['Peak_0178','Peak_0231'])\n",
    "\n",
    "roc_fn = roc_plots[2][-1]\n",
    "box_plot_fn = box_plots[0][-1]\n",
    "if dt_plots:\n",
    "    dt_fn = Path(dt_plots[1][-1][-1])\n",
    "\n",
    "ts_fn0 = ts_plots[1][0]\n",
    "ts_fn1 = ts_plots[1][1]\n",
    "    \n",
    "# display images for the TOPHAT method\n",
    "images = [roc_fn, box_plot_fn, dt_fn, ts_fn0, ts_fn1]\n",
    "\n",
    "for fn in images:\n",
    "    display(Image(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Prediction\n",
    "A `PredictionModel` is created, which trains a random forest classifier on the best performing features for both performance measures and the complete training set.\n",
    "Subsequently, it is initialized with the raw validation samples. Therefore, it needs to perform the same normalization, denoising, peak-detection, and alignment steps as performed in the training procedure before.\n",
    "\n",
    "After this, it reduces the resulting feature matrices from the validation set to the ones present in the training matrices. Lastly, it predicts the classes for the validation samples and returns the classification results. These, in turn, are then used as the final results and lead to the evaluation of the validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# prediction - will use parameters and features from previous steps\n",
    "predictionModel = PredictionModel(ims_analysis)\n",
    "\n",
    "#  preparation - replace train_ with test_\n",
    "#  otherwise can't find measurements - as the class labels don't match the measurement names\n",
    "file_parameters['folder_path'] = file_parameters['folder_path'].replace(\"train_\", \"test_\")\n",
    "test_dir = file_parameters['folder_path']\n",
    "\n",
    "test_result_dir = file_parameters['out_dir'].replace(\"train_\", \"test_\")\n",
    "\n",
    "test_measurements_fns = sorted(Path(test_dir).glob(\"*ims.csv\"))\n",
    "test_measurements = [MccImsMeasurement(fn) for fn in test_measurements_fns]\n",
    "\n",
    "# predict - and run full preprocessing and alignment on test_measurements\n",
    "prediction = predictionModel.predict(test_measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of Prediction Model\n",
    "For each of the included peak-detection methods the `PredictionModel` was applied to the samples in the validation set. Here the correct and predicted labels are compared, listing the correctly and misidentified class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dict_fn = MccImsAnalysis.guess_class_label_extension(test_dir)\n",
    "test_labels_dict = MccImsAnalysis.parse_class_labels(test_labels_dict_fn)\n",
    "class_labels = np.unique([m.class_label for m in ims_analysis.measurements])\n",
    "test_measurements_names = [path.name for path in test_measurements_fns]\n",
    "for pdm, prediction_index in prediction.items():\n",
    "    predicted_labels = {test_name: class_labels[p] for p, test_name in zip(prediction_index, test_measurements_names)}\n",
    "    correct = dict()\n",
    "    false = dict()\n",
    "    for fn, predicted_label in predicted_labels.items():\n",
    "        if predicted_label == test_labels_dict[fn]:\n",
    "            correct[fn] = predicted_label\n",
    "        else:\n",
    "            false[fn] = predicted_label\n",
    "\n",
    "    print(f\"resulting_labels for {pdm.name} are: {predicted_labels}\")\n",
    "    print(f\"Falsely classified: {false}\\n\")\n",
    "    print(f\"That's {len(correct.keys())} correct vs {len(false.keys())} false\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set ROC-Curve\n",
    "Finally a ROC-curve is plotted and shown for the performance of the prediction model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a ROC curve for the predictions\n",
    "roc_pred_plots = RocCurvePlot.ROCCurveFromPrediction(\n",
    "        prediction_model = predictionModel, \n",
    "        prediction_class_label_dict = test_labels_dict, \n",
    "        plot_parameters=plot_parameters)\n",
    "roc_pred_fn = roc_pred_plots[2][-1]\n",
    "display(Image(roc_pred_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "> P. Weber, J. K. Pauling, M. List, and J. Baumbach, “BALSAM—An Interactive Online Platformfor Breath Analysis, Visualization and Classification”, Metabolites 10, 393 (2020) [Weber et al. 2020][Weber2020].\n",
    "\n",
    "> J. Engel, J. Gerretzen, E. Szymanska, J. J. Jansen, G. Downey, L. Blanchet, and L. M. Buydens,“Breaking with trends in pre-processing?”, TrAC Trends Anal. Chem.50, 96–106 (2013) [Engel et al. 2013][Engel2013].\n",
    "\n",
    "> A. Smolinska, A.-C. Hauschild, R. R. R. Fijten, J. W. Dallinga, J. Baumbach, and F. J. vanSchooten, “Current breathomics—a review on data pre-processing techniques and machinelearning in metabolomics breath analysis”, J. Breath Res.8, 027105 (2014) [Smolinska et al. 2014][Smolinska2014].\n",
    "\n",
    "> E. Szymanska, G. H. Tinnevelt, E. Brodrick, M. Williams, A. N. Davies, H.-J. van Manen, andL. M. Buydens, “Increasing conclusiveness of clinical breath analysis by improved baselinecorrection of multi capillary column – ion mobility spectrometry (MCC-IMS) data”, J. Pharm. Biomed. Analysis127, 170–175 (2016) [Szymanska et al. 2016][Szymanska2016].\n",
    "\n",
    "> G. Lee, R. Gommers, F. Waselewski, K. Wohlfahrt, and A. O’Leary, “PyWavelets: A Pythonpackage for wavelet analysis”, J. Open Source Softw.4, 1237 (2019) [Lee et al. 2019][Lee2019].\n",
    "\n",
    "> P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski,P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman,N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W.Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero,C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, and P. van Mulbregt, “SciPy1.0: fundamental algorithms for scientific computing in Python”, Nat. Methods17, 261–272(2020) [Virtanen et al. 2020][Virtanen2020].\n",
    "\n",
    "> M. D’Addario, D. Kopczynski, J. Baumbach, and S. Rahmann, “A modular computationalframework for automated peak extraction from ion mobility spectra”, BMC Bioinforma.15,25 (2014) [D'Addario et al. 2014][DAddario2014].\n",
    "\n",
    "> A. Bunkowski, “MCC-IMS data analysis using automated spectra processing and explorativevisualisation methods”, Ph.D. thesis, Bielefeld University (2011) [Bunkowski et al. 2019][Bunkowski2019].\n",
    "\n",
    "> B. Bödeker, W. Vautz, and J. I. Baumbach, “Peak finding and referencing in MCC/IMS-data”, Int J Ion Mobil. Spectrom 11(2008) [Bödeker et al. 2008][Bödeker2008].\n",
    "\n",
    "> S. Sternberg,  “Grayscale morphology”, Comput. vision,  graphics,  image processing pp.333–355 (1986) [Sternberg et al. 1986][Sternberg1986].\n",
    "\n",
    "> S. Horsch,  D. Kopczynski,  E. Kuthe,  J. I. Baumbach,  S. Rahmann,  and J. Rahnenführer, “A  detailed  comparison  of  analysis  processes  for  MCC-IMS  data  in  disease  classifica-tion—Automated methods can replace manual peak annotations”, PLOS ONE 12, (2017) [Horsch et al. 2017][Horsch2017].\n",
    "\n",
    "> H. B. Mann and D. R. Whitney, “On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other”, The Annals Math. Stat. 18, 50–60 (1947) [Mann and Whitney 1947][Mann1947].\n",
    "\n",
    "> Y.  Benjamini  and  Y.  Hochberg,  “Controlling the  False  Discovery  Rate:  A  Practical  and Powerful Approach to Multiple Testing”, J. Royal Stat. Soc. Ser. B (Methodological) 57,289–300 (1995) [Benjamini and Hochberg 1995][Benjamini1995].\n",
    "\n",
    "> S. Seabold and J. Perktold, “Statsmodels: Econometric and Statistical Modeling with Python”, in PROC. OF THE 9th PYTHON IN SCIENCE CONF (2010) [Seabold and Perktold 2010][Seabold2010].\n",
    "\n",
    "\n",
    "[Weber2020]: https://doi.org/10.3390/metabo10100393\n",
    "[Smolinska2014]: https://doi.org/10.1088/1752-7155/8/2/027105\n",
    "[Szymanska2016]: https://doi.org/10.1039/C6AN01008C\n",
    "[Lee2019]: https://doi.org/10.21105/joss.01237\n",
    "[Horsch2017]: https://doi.org/10.1371/journal.pone.0184321\n",
    "[Engel2013]: https://doi.org/10.1016/j.trac.2013.04.015\n",
    "[Virtanen2020]: https://doi.org/10.1038/s41592-019-0686-2\n",
    "[DAddario2014]: https://doi.org/10.1186/1471-2105-15-25\n",
    "[Bunkowski2019]:https://pub.uni-bielefeld.de/publication/2517237\n",
    "[Bödeker2008]: https://doi.org/10.1007/s12127-008-0012-7\n",
    "[Sternberg1986]: http://www.sciencedirect.com/science/article/pii/0734189X86900046\n",
    "[Ester1996]: http://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf\n",
    "[Mann1947]: https://doi.org/10.1214/aoms/1177730491\n",
    "[Benjamini1995]: https://doi.org/10.1111/j.2517-6161.1995.tb02031.x\n",
    "[Seabold2010]: http://statsmodels.sourceforge.net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
